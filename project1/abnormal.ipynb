{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f47294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Autoencoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "# ---------------------------------------------------------\n",
    "df = pd.read_csv(\"../data/USD_KRW.csv\")\n",
    "df = df.loc[df['date'] >= '1997.12.16', ['date', 'close']]\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# ìˆ˜ìµë¥  ê³„ì‚°\n",
    "df['return'] = df['close'].shift() / df['close'].shift(5) - 1\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. ì‹œê³„ì—´ ìœˆë„ìš° ë§Œë“¤ê¸° í•¨ìˆ˜\n",
    "# ---------------------------------------------------------\n",
    "def create_sequences(data, window_size):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        seq = data[i:i+window_size]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš©\n",
    "# ---------------------------------------------------------\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "# ìœˆë„ìš° ì ìš©ì„ ìœ„í•´ returnë§Œ ì¶”ì¶œ\n",
    "returns = df[['return']].values\n",
    "sequences = create_sequences(returns, WINDOW_SIZE)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "# ---------------------------------------------------------\n",
    "dates = df.index[WINDOW_SIZE:]  # ì‹œí€€ìŠ¤ì˜ ë‚ ì§œëŠ” WINDOW_SIZE ì´í›„ë¶€í„° ê°€ëŠ¥\n",
    "assert len(sequences) == len(dates)\n",
    "\n",
    "train_end = pd.Timestamp(\"2018.01.01\")\n",
    "test_start = pd.Timestamp(\"2022.01.01\")\n",
    "\n",
    "# ê° ì„¸íŠ¸ì˜ ì¸ë±ìŠ¤ êµ¬í•˜ê¸°\n",
    "train_idx = dates < train_end\n",
    "test_idx = dates >= test_start\n",
    "val_idx = (dates >= train_end) & (dates < test_start)\n",
    "\n",
    "# ë¶„í• \n",
    "x_train = sequences[train_idx]\n",
    "x_val = sequences[val_idx]\n",
    "x_test = sequences[test_idx]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. ì •ê·œí™” (train ê¸°ì¤€ìœ¼ë¡œ fit)\n",
    "# ---------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 2Dë¡œ reshape í›„ ì •ê·œí™”\n",
    "x_train_2d = x_train.reshape(-1, WINDOW_SIZE)\n",
    "x_val_2d = x_val.reshape(-1, WINDOW_SIZE)\n",
    "x_test_2d = x_test.reshape(-1, WINDOW_SIZE)\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train_2d).reshape(-1, WINDOW_SIZE, 1)\n",
    "x_val_scaled = scaler.transform(x_val_2d).reshape(-1, WINDOW_SIZE, 1)\n",
    "x_test_scaled = scaler.transform(x_test_2d).reshape(-1, WINDOW_SIZE, 1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. LSTM Autoencoder ì •ì˜\n",
    "# ---------------------------------------------------------\n",
    "LATENT_DIM = 16\n",
    "\n",
    "input_layer = Input(shape=(WINDOW_SIZE, 1))\n",
    "encoded = LSTM(LATENT_DIM, activation='relu', return_sequences=False)(input_layer)\n",
    "decoded = RepeatVector(WINDOW_SIZE)(encoded)\n",
    "decoded = LSTM(1, activation='linear', return_sequences=True)(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. ëª¨ë¸ í•™ìŠµ\n",
    "# ---------------------------------------------------------\n",
    "history = autoencoder.fit(\n",
    "    x_train_scaled, x_train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val_scaled, x_val_scaled),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚°\n",
    "# ---------------------------------------------------------\n",
    "x_test_pred = autoencoder.predict(x_test_scaled)\n",
    "mse = np.mean(np.power(x_test_scaled - x_test_pred, 2), axis=(1, 2))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 9. ì´ìƒ íƒì§€ (Threshold ìë™í™”: IQR ë°©ì‹ ì˜ˆì‹œ)\n",
    "# ---------------------------------------------------------\n",
    "q1 = np.percentile(mse, 25)\n",
    "q3 = np.percentile(mse, 75)\n",
    "iqr = q3 - q1\n",
    "threshold = q3 + 1.5 * iqr\n",
    "\n",
    "# ì´ìƒ íƒì§€\n",
    "anomalies = mse > threshold\n",
    "anomaly_dates = dates[test_idx][anomalies]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 10. ì‹œê°í™”\n",
    "# ---------------------------------------------------------\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "mse_series = pd.Series(mse, index=dates[test_idx])\n",
    "\n",
    "# í™˜ìœ¨ ì‹œê³„ì—´ì—ì„œ í…ŒìŠ¤íŠ¸ êµ¬ê°„ë§Œ ì¶”ì¶œ\n",
    "price_series = df.loc[dates[test_idx], 'close']\n",
    "\n",
    "# ì´ìƒì¹˜ ì‹œì ì— í•´ë‹¹í•˜ëŠ” í™˜ìœ¨ ê°’\n",
    "anomaly_prices = price_series.loc[anomaly_dates]\n",
    "\n",
    "# ê·¸ë˜í”„ ì¶œë ¥\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(price_series, label='Exchange Rate', color='blue')\n",
    "plt.scatter(anomaly_dates, anomaly_prices, color='red', marker='x', label='Anomaly')\n",
    "plt.title(\"Exchange Rate with Anomalies Detected by LSTM Autoencoder\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Exchange Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551718cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- ë°ì´í„° ë° ì „ì²˜ë¦¬ ---\n",
    "# df: ë‚ ì§œ ì¸ë±ìŠ¤ í¬í•¨ DataFrame, data: íŠ¹ì§• ì»¬ëŸ¼ë§Œ ìˆëŠ” DataFrame\n",
    "df = pd.read_csv('../data/USD_KRW.csv')\n",
    "df = df.loc[df['date'] >= '1997.12.16', : ]\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "data = df.copy()\n",
    "\n",
    "X = data.values  # (samples, features)\n",
    "dates = data.index\n",
    "\n",
    "# -------------------------\n",
    "# 3. íŠ¸ë Œë“œ ì œê±° (í™˜ìœ¨ì— ëŒ€í•´)\n",
    "# -------------------------\n",
    "rolling_window = 20\n",
    "data['close_detrended'] = data['close'] - data['close'].rolling(window=rolling_window, min_periods=1).mean()\n",
    "\n",
    "# -------------------------\n",
    "# 4. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "# -------------------------\n",
    "test_start = pd.Timestamp(\"2022.01.01\")\n",
    "train_data = data.loc[data.index < test_start]\n",
    "test_data = data.loc[data.index >= test_start]\n",
    "\n",
    "# -------------------------\n",
    "# 5. íŠ¹ì§• ì„ íƒ ë° ìŠ¤ì¼€ì¼ë§\n",
    "# -------------------------\n",
    "features = data.columns.drop('close')  # 'ì¢…ê°€'ëŠ” ì˜ˆì¸¡ ëŒ€ìƒì´ë¼ ì œì™¸ (ë˜ëŠ” í¬í•¨í•´ë„ ë¨)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data[features])\n",
    "X_test = scaler.transform(test_data[features])\n",
    "\n",
    "# -------------------------\n",
    "# 6. Isolation Forest í›ˆë ¨\n",
    "# -------------------------\n",
    "iso_forest = IsolationForest(contamination=0.02, random_state=42)\n",
    "iso_forest.fit(X_train)\n",
    "\n",
    "# ì˜ˆì¸¡ (1: ì •ìƒ, -1: ì´ìƒ)\n",
    "y_pred = iso_forest.predict(X_test)\n",
    "anomalies = y_pred == -1\n",
    "\n",
    "# -------------------------\n",
    "# 7. ì‹œê°í™”\n",
    "# -------------------------\n",
    "test_dates = test_data.index\n",
    "price_series = test_data['close']\n",
    "anomaly_dates = test_dates[anomalies]\n",
    "anomaly_prices = price_series.loc[anomaly_dates]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(test_dates, price_series, label='Exchange Rate', color='blue')\n",
    "plt.scatter(anomaly_dates, anomaly_prices, color='red', marker='x', label='Isolation Forest Anomaly')\n",
    "plt.title(\"Isolation Forest Anomaly Detection on Exchange Rate (Detrended)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Exchange Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57570f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# --- ë°ì´í„° ë° ì „ì²˜ë¦¬ ---\n",
    "# df: ë‚ ì§œ ì¸ë±ìŠ¤ í¬í•¨ DataFrame, data: íŠ¹ì§• ì»¬ëŸ¼ë§Œ ìˆëŠ” DataFrame\n",
    "df = pd.read_csv('../data/USD_KRW.csv')\n",
    "df = df.loc[df['ë‚ ì§œ'] >= '1997.12.16', : ]\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Trend removal: 5-day moving average\n",
    "df_trend = df['close'].rolling(window=5, min_periods=1, center=True).mean()\n",
    "df_detrended = df.copy()\n",
    "df_detrended['close'] = data['close'] - df_trend\n",
    "\n",
    "# Scale all features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_detrended)\n",
    "\n",
    "# Train One-Class SVM\n",
    "svm = OneClassSVM(nu=0.05, kernel='rbf', gamma='scale')\n",
    "svm.fit(data_scaled)\n",
    "\n",
    "# Predict\n",
    "pred = svm.predict(data_scaled)\n",
    "anomalies = pred == -1\n",
    "anomaly_dates = df.index[anomalies]\n",
    "anomaly_prices = df.loc[anomalies, 'close']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(df.index, df['close'], label='Exchange Rate')\n",
    "plt.scatter(anomaly_dates, anomaly_prices, color='red', marker='x', label='One-Class SVM Anomaly')\n",
    "plt.title(\"One-Class SVM Anomaly Detection on Exchange Rate\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Exchange Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06673897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Z-score ê¸°ë°˜ ì´ìƒ íƒì§€ íŒŒì´í”„ë¼ì¸\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from scipy.stats import zscore\n",
    "import seaborn as sns\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv('/content/drive/MyDrive/á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„‹á…µá†¯á„‹á…µá†¯á„’á…ªá†«á„‹á…²á†¯.csv')\n",
    "df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])\n",
    "df.set_index('ë‚ ì§œ', inplace=True)\n",
    "\n",
    "# ë³€ë™ë¥ (ë¡œê·¸ ìˆ˜ìµë¥ ) ê³„ì‚°\n",
    "df['return'] = np.log(df['ì¢…ê°€']).diff(-1)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Z-score ê³„ì‚°\n",
    "df['z_score'] = zscore(df['return'])\n",
    "\n",
    "# âœ… Threshold ìë™ ì„¤ì • (IQR ë°©ì‹)\n",
    "q1 = df['z_score'].quantile(0.25)\n",
    "q3 = df['z_score'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "upper = q3 + 1.5 * iqr\n",
    "lower = q1 - 1.5 * iqr\n",
    "\n",
    "# ì´ìƒì¹˜ íŒë‹¨\n",
    "df['anomaly'] = ((df['z_score'] > upper) | (df['z_score'] < lower)).astype(int)\n",
    "\n",
    "# âœ… ë¼ë²¨ ìƒì„± (ì‹¤ì œ ë¼ë²¨ì´ ìˆë‹¤ë©´ ì´ ë¶€ë¶„ ëŒ€ì²´ ê°€ëŠ¥)\n",
    "# ìˆ˜ìµë¥ ì´ Â±1.5% ì´ìƒì´ë©´ ì´ìƒì¹˜ë¡œ ê°„ì£¼\n",
    "threshold_label = 0.015\n",
    "df['label'] = (np.abs(df['return']) > threshold_label).astype(int)\n",
    "\n",
    "# âœ… í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
    "accuracy = accuracy_score(df['label'], df['anomaly'])\n",
    "precision = precision_score(df['label'], df['anomaly'])\n",
    "recall = recall_score(df['label'], df['anomaly'])\n",
    "f1 = f1_score(df['label'], df['anomaly'])\n",
    "\n",
    "print(\"ğŸ“Š accuracy:\", round(accuracy, 3))\n",
    "print(\"ğŸ“Š Precision:\", round(precision, 3))\n",
    "print(\"ğŸ“Š Recall:\", round(recall, 3))\n",
    "print(\"ğŸ“Š F1 Score:\", round(f1, 3))\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(confusion_matrix(df['label'], df['anomaly']), annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# âœ… ì‹œê°í™”\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(df.index, df['ì¢…ê°€'], label='Exchange Rate')\n",
    "plt.scatter(df.index[df['anomaly'] == 1], df['ì¢…ê°€'][df['anomaly'] == 1], color='red', marker='x', label='Detected Anomaly')\n",
    "plt.title(\"ğŸ“ˆ Z-score ê¸°ë°˜ í™˜ìœ¨ ì´ìƒ íƒì§€ ê²°ê³¼\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Exchange Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Score ê¸°ë°˜ ì˜ˆì¸¡\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# === íŒŒë¼ë¯¸í„° ===\n",
    "window_size = 20      # ê³¼ê±° ë©°ì¹  ë°ì´í„°ë¥¼ inputìœ¼ë¡œ ì‚¬ìš©í• ì§€\n",
    "future_window = 5     # í–¥í›„ ë©°ì¹  ì¤‘ ì´ìƒì¹˜ í™•ì¸\n",
    "z_thresh = 1.5       # ì´ìƒì¹˜ ê¸°ì¤€\n",
    "\n",
    "# === ë°ì´í„° ë¡œë”© ===\n",
    "df = pd.read_csv('../data/á„ƒá…©á†¨á„…á…µá†¸á„‡á…§á†«á„‰á…®.csv')\n",
    "\n",
    "# === ê¸°ë³¸ í”¼ì²˜ ===\n",
    "df['return'] = df['ì¢…ê°€'].pct_change()\n",
    "df['zscore'] = (df['ì¢…ê°€'] - df['ì¢…ê°€'].rolling(20).mean()) / df['ì¢…ê°€'].rolling(20).std()\n",
    "\n",
    "# === RSI ê³„ì‚° í•¨ìˆ˜ ===\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(period).mean()\n",
    "    avg_loss = loss.rolling(period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# === í–¥í›„ ì´ìƒì¹˜ ì—¬ë¶€ íƒ€ê²Ÿ ìƒì„± ===\n",
    "def label_future_outliers(z_scores, window, threshold):\n",
    "    labels = []\n",
    "    for i in range(len(z_scores)):\n",
    "        future_window = z_scores[i+1:i+1+window]\n",
    "        if future_window.isna().any() or len(future_window) < window:\n",
    "            labels.append(np.nan)\n",
    "        else:\n",
    "            labels.append(int((future_window.abs() > threshold).any()))\n",
    "    return labels\n",
    "\n",
    "df['is_outlier_future'] = label_future_outliers(df['zscore'], future_window, z_thresh)\n",
    "\n",
    "# === ì¶”ê°€ í”¼ì²˜ë“¤ ===\n",
    "df['rolling_mean_5'] = df['ì¢…ê°€'].rolling(5).mean()\n",
    "df['rolling_std_5'] = df['ì¢…ê°€'].rolling(5).std()\n",
    "df['ema_5'] = df['ì¢…ê°€'].ewm(span=5).mean()\n",
    "df['bollinger_upper'] = df['rolling_mean_5'] + 2 * df['rolling_std_5']\n",
    "df['bollinger_lower'] = df['rolling_mean_5'] - 2 * df['rolling_std_5']\n",
    "df['zscore_return'] = (df['return'] - df['return'].rolling(30).mean()) / df['return'].rolling(30).std()\n",
    "df['momentum_5'] = df['ì¢…ê°€'] - df['ì¢…ê°€'].shift(5)\n",
    "df['rsi_14'] = compute_rsi(df['ì¢…ê°€'], 14)\n",
    "\n",
    "# === Lag í”¼ì²˜ ===\n",
    "for i in range(1, window_size + 1):\n",
    "    df[f'lag_{i}'] = df['ì¢…ê°€'].shift(i)\n",
    "    df[f'return_lag_{i}'] = df['return'].shift(i)\n",
    "\n",
    "features = df.columns[1 : ]\n",
    "\n",
    "\n",
    "# === í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„ ===\n",
    "df_model = df.dropna(axis=0)\n",
    "X = df_model[features].drop('is_outlier_future', axis=1)\n",
    "y = df_model['is_outlier_future']\n",
    "\n",
    "# === Train/Test Split ===\n",
    "X_train, X_test, y_train, y_test = X[ : int(len(X) * 0.7)], X[int(len(X) * 0.7) : ], y[ : int(len(X) * 0.7)], y[int(len(X) * 0.7) : ]\n",
    "\n",
    "# === í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì • ===\n",
    "pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)\n",
    "\n",
    "# === XGBoost ëª¨ë¸ ===\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=pos_weight,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === í‰ê°€ ===\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# === í”¼ì²˜ ì¤‘ìš”ë„ ì¶œë ¥ (ì„ íƒ ì‚¬í•­) ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importances.sort_values(ascending=False).head(15), y=feature_importances.sort_values(ascending=False).head(15).index)\n",
    "plt.title(\"Top 15 Feature Importances\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
